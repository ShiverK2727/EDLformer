import os
import sys
import yaml
import torch
import random
import numpy as np
import datetime
from typing import List, Dict, Tuple, Any
from torch.utils.data import DataLoader
from collections import deque
import shutil
import json
import argparse

# --- [æ ¸å¿ƒä¿®æ”¹] å¯¼å…¥æ‰€æœ‰éœ€è¦çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬æ–°çš„ç»Ÿä¸€æ¨¡å‹å’Œæ—§æ¨¡å‹ ---
# from nets.simple_maskformer import SimpleMaskFormer, SimpleMaskFormerMulti
from nets.simple_edl_maskformer import FlexibleEDLMaskFormer

# --- [æ ¸å¿ƒä¿®æ”¹] å¯¼å…¥ä¸¤ç§æ¨¡å¼å¯¹åº”çš„EDLæŸå¤±å‡½æ•° ---
# from scheduler.edl_single_loss import EDLSingleLoss
# from scheduler.simple_maskformer_loss import SimpleMaskformerLoss
# from scheduler.simple_maskformer_multi_loss import SimpleMaskFormerMultiLoss
from scheduler.simple_edl_single_maskformer_loss import SimpleEDLMaskformerLossV2

# å¯¼å…¥æ•°æ®é›†å’Œå…¶ä»–è¾…åŠ©æ¨¡å—
from datasets import RIGADatasetSimpleV2
from logger import log_info, log_error

# ==============================================================================
# åŸºç¡€è®¾ç½® (Setup)
# ==============================================================================

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒçš„å¯é‡å¤æ€§ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    log_info(f"éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}", print_message=False)

def load_config(yaml_path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶ã€‚"""
    with open(yaml_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

# ==============================================================================
# [æ ¸å¿ƒä¿®æ”¹] ç»Ÿä¸€çš„ç»„ä»¶æ„å»ºå™¨
# ==============================================================================

def build_edl_components(config):
    """
    [ç»Ÿä¸€æ„å»ºå™¨] æ ¹æ®æ–°çš„é…ç½®ç»“æ„æ„å»ºæ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®é›†ã€‚
    """
    model_config = config['model']
    loss_config = config.get('loss', {})
    training_config = config.get('training', {})
    dataset_config = config.get('dataset', {})
    
    # å‚æ•°éªŒè¯
    required_model_params = ['num_cls_classes', 'num_queries']
    for param in required_model_params:
        if param not in model_config:
            raise ValueError(f"æ¨¡å‹é…ç½®ä¸­ç¼ºå°‘å¿…éœ€å‚æ•°: {param}")
    
    # 1. æ„å»ºæ¨¡å‹
    log_info("="*80, print_message=True)
    log_info("æ„å»º FlexibleEDLMaskFormer æ¨¡å‹...", print_message=True)
    log_info(f"æ¨¡å‹é…ç½®: {json.dumps(model_config, indent=2)}", print_message=False)
    model = FlexibleEDLMaskFormer(**model_config)
    log_info("æ¨¡å‹æ„å»ºå®Œæˆã€‚", print_message=True)

    # 2. æ„å»ºæŸå¤±å‡½æ•°
    log_info("="*80, print_message=True)
    log_info("æ„å»º SimpleEDLMaskformerLossV2 æŸå¤±å‡½æ•°...", print_message=True)
    loss_params = {k: v for k, v in loss_config.items()}
    log_info(f"æŸå¤±å‡½æ•°é…ç½®: {json.dumps(loss_params, indent=2)}", print_message=False)
    loss_fn = SimpleEDLMaskformerLossV2(**loss_params)
    log_info("æŸå¤±å‡½æ•°æ„å»ºå®Œæˆã€‚", print_message=True)

    # 3. æ„å»ºæ•°æ®é›†
    log_info("="*80, print_message=True)
    log_info("æ„å»ºæ•°æ®é›†...", print_message=True)
    
    # ä»æ–°çš„é…ç½®ç»“æ„ä¸­è·å–æ•°æ®é›†é…ç½®
    dataset_yaml = dataset_config.get('config_path') or config.get('dataset_yaml')  # å…¼å®¹æ—§é…ç½®
    if not dataset_yaml or not os.path.exists(dataset_yaml):
        raise FileNotFoundError(f"æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {dataset_yaml}")
    
    train_dataset = RIGADatasetSimpleV2(config_path=dataset_yaml, is_train=True)
    val_dataset = RIGADatasetSimpleV2(config_path=dataset_yaml, is_train=False)
    
    # ä»trainingé…ç½®ä¸­è·å–æ‰¹å¤„ç†å¤§å°
    batch_size = training_config.get('batch_size', 8)
    num_workers = training_config.get('num_workers', 4)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True)
    log_info(f"æ•°æ®é›†æ„å»ºå®Œæˆã€‚è®­ç»ƒé›†: {len(train_dataset)}æ ·æœ¬, éªŒè¯é›†: {len(val_dataset)}æ ·æœ¬ã€‚", print_message=True)
    log_info(f"æ‰¹å¤„ç†å¤§å°: {batch_size}, å·¥ä½œçº¿ç¨‹æ•°: {num_workers}", print_message=True)
    log_info("="*80, print_message=True)

    return model.cuda(), loss_fn, train_loader, val_loader


def build_complete_training_setup(config, args=None, save_override_record_flag=True):
    """
    æ„å»ºå®Œæ•´çš„è®­ç»ƒè®¾ç½®ï¼ŒåŒ…æ‹¬æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€æ•°æ®é›†ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ã€‚
    
    Args:
        config: é…ç½®å­—å…¸
        args: å‘½ä»¤è¡Œå‚æ•°ï¼ˆå¯é€‰ï¼‰
        save_override_record_flag: æ˜¯å¦ä¿å­˜è¦†å†™è®°å½•
    
    Returns:
        tuple: (model, loss_fn, train_loader, val_loader, optimizer, scheduler)
    """
    log_info("="*80, print_message=True)
    log_info("æ„å»ºå®Œæ•´è®­ç»ƒè®¾ç½®", print_message=True)
    log_info("="*80, print_message=True)
    
    # å¤„ç†å‚æ•°è¦†å†™
    override_params = {}
    if args is not None:
        original_config = config.copy()
        config, override_params = apply_args_override(config, args)
        
        # æ˜¾ç¤ºè¦†å†™æ‘˜è¦
        print_override_summary(override_params)
        
        # ä¿å­˜è¦†å†™è®°å½•
        if save_override_record_flag and override_params:
            save_dir = config.get('experiment', {}).get('save_dir', './exp/default')
            save_override_record(config, override_params, args, save_dir)
    
    # è®¾ç½®éšæœºç§å­
    training_config = config.get('training', {})
    seed = training_config.get('seed', 42)
    set_seed(seed)
    
    # 1. æ„å»ºæ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®é›†
    model, loss_fn, train_loader, val_loader = build_edl_components(config)
    
    # 2. æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    log_info("æ„å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨...", print_message=True)
    optimizer, scheduler = build_optimizer_and_scheduler(model, config)
    
    log_info("="*80, print_message=True)
    log_info("å®Œæ•´è®­ç»ƒè®¾ç½®æ„å»ºå®Œæˆ", print_message=True)
    log_info(f"ğŸ¯ å®éªŒä¿å­˜ç›®å½•: {config.get('experiment', {}).get('save_dir', 'N/A')}", print_message=True)
    log_info("="*80, print_message=True)
    
    return model, loss_fn, train_loader, val_loader, optimizer, scheduler


def build_optimizer_and_scheduler(model, config):
    """ç»Ÿä¸€æ„å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œä½¿ç”¨å•ä¸€å­¦ä¹ ç‡é…ç½®ã€‚"""
    training_cfg = config.get('training', {})
    
    # ç»Ÿä¸€å­¦ä¹ ç‡é…ç½® - å¼ºåˆ¶ä½¿ç”¨å•ä¸ªå­¦ä¹ ç‡å£°æ˜
    base_lr = float(training_cfg.get('learning_rate', 1e-3))
    log_info(f"ä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡é…ç½®: {base_lr}", print_message=True)
    
    # ä¼˜åŒ–å™¨é…ç½®
    optimizer_type = training_cfg.get('optimizer_type', 'AdamW')
    weight_decay = float(training_cfg.get('weight_decay', 1e-4))
    
    # æ„å»ºä¼˜åŒ–å™¨
    if optimizer_type.lower() == 'adamw':
        betas = training_cfg.get('betas', [0.9, 0.999])
        if isinstance(betas, list) and len(betas) == 2:
            betas = (float(betas[0]), float(betas[1]))
        
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=base_lr,
            weight_decay=weight_decay,
            betas=betas,
            eps=float(training_cfg.get('eps', 1e-8)),
            amsgrad=bool(training_cfg.get('amsgrad', False))
        )
        log_info(f"æ„å»ºAdamWä¼˜åŒ–å™¨: lr={base_lr}, weight_decay={weight_decay}", print_message=False)
        
    elif optimizer_type.lower() == 'adam':
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=base_lr,
            weight_decay=weight_decay
        )
        log_info(f"æ„å»ºAdamä¼˜åŒ–å™¨: lr={base_lr}, weight_decay={weight_decay}", print_message=False)
        
    elif optimizer_type.lower() == 'sgd':
        momentum = float(training_cfg.get('momentum', 0.9))
        optimizer = torch.optim.SGD(
            model.parameters(),
            lr=base_lr,
            weight_decay=weight_decay,
            momentum=momentum
        )
        log_info(f"æ„å»ºSGDä¼˜åŒ–å™¨: lr={base_lr}, weight_decay={weight_decay}, momentum={momentum}", print_message=False)
    else:
        raise ValueError(f"æœªçŸ¥çš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
    use_scheduler = training_cfg.get('use_cosine_scheduler', False)
    if not use_scheduler:
        log_info("ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚", print_message=False)
        return optimizer, None
    
    # æ„å»ºä½™å¼¦é€€ç«è°ƒåº¦å™¨
    total_epochs = config.get('epochs', 100)
    warmup_epochs = int(training_cfg.get('warmup_epochs', 0))
    min_lr = float(training_cfg.get('min_learning_rate', 1e-6))
    
    def lr_lambda(epoch):
        if warmup_epochs > 0 and epoch < warmup_epochs:
            # çº¿æ€§warmup
            return float(epoch + 1) / float(max(1, warmup_epochs))
        
        # ä½™å¼¦é€€ç«
        progress = float(epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))
        progress = min(max(progress, 0.0), 1.0)
        cosine = 0.5 * (1. + np.cos(np.pi * progress))
        return cosine * (1.0 - min_lr / base_lr) + min_lr / base_lr
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    log_info(f"æ„å»ºä½™å¼¦é€€ç«è°ƒåº¦å™¨: warmup_epochs={warmup_epochs}, min_lr={min_lr}", print_message=False)
    
    return optimizer, scheduler


def process_batch_for_expert_class_combination(
    batch: Dict[str, torch.Tensor]
) -> Tuple[List[Dict[str, torch.Tensor]], Dict[str, Any]]:
    """
    å¤„ç†æ¥è‡ªå¤šä¸“å®¶æ ‡æ³¨æ•°æ®é›†çš„æ‰¹æ¬¡æ•°æ®ï¼Œå°†(ä¸“å®¶, ç±»åˆ«)ç»„åˆè§†ä¸ºæ–°ç±»åˆ«ã€‚

    æ­¤å‡½æ•°æ—¨åœ¨ä¸ºç±» MaskFormer æ¨¡å‹å‡†å¤‡ 'targets'ï¼ŒåŒæ—¶ç”Ÿæˆç”¨äºç»“æœé‡æ„çš„å…ƒæ•°æ®ã€‚

    Args:
        batch (Dict[str, torch.Tensor]): 
            ä» DataLoader è¾“å‡ºçš„æ‰¹æ¬¡å­—å…¸ï¼Œè‡³å°‘éœ€è¦åŒ…å«:
            - 'expert_masks': torch.Tensor, å½¢çŠ¶ä¸º [B, N, C, H, W]
              B: æ‰¹æ¬¡å¤§å°, N: ä¸“å®¶æ•°, C: åŸå§‹ç±»åˆ«æ•°, H, W: æ©ç é«˜å®½ã€‚
              æ©ç åº”ä¸ºäºŒå€¼ (0 æˆ– 1)ã€‚

    Returns:
        Tuple[List[Dict[str, torch.Tensor]], Dict[str, Any]]:
        - targets (List[Dict[str, torch.Tensor]]):
            ä¸€ä¸ªåˆ—è¡¨ï¼Œé•¿åº¦ä¸ºBã€‚æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«:
            - 'labels': 1D Tensor, å½¢çŠ¶ä¸º [N*C]ï¼ŒåŒ…å«æ–°ç”Ÿæˆçš„å”¯ä¸€ç±»åˆ«IDã€‚
            - 'masks': 3D Tensor, å½¢çŠ¶ä¸º [N*C, H, W]ï¼ŒåŒ…å«å¯¹åº”çš„äºŒå€¼æ©ç ã€‚
            æ­¤æ ¼å¼å¯ç›´æ¥ç”¨äº DETR/MaskFormer ç±»çš„æŸå¤±å‡½æ•°ã€‚

        - metadata (Dict[str, Any]):
            ä¸€ä¸ªå…ƒæ•°æ®å­—å…¸ï¼Œç”¨äºåç»­ç»“æœè§£æå’Œé‡æ„ï¼ŒåŒ…å«:
            - 'num_experts' (int): åŸå§‹ä¸“å®¶æ•° Nã€‚
            - 'num_classes' (int): åŸå§‹ç±»åˆ«æ•° Cã€‚
            - 'total_combined_classes' (int): N * Cï¼Œç»„åˆåçš„æ€»ç±»åˆ«æ•°ã€‚
            - 'mapping_tensor': 2D Tensor, å½¢çŠ¶ä¸º [N*C, 2]ã€‚
              æ¯ä¸€è¡Œ `mapping_tensor[new_label]` åŒ…å«äº† `[expert_id, class_id]`ï¼Œ
              å¯ä»¥å°†æ–°çš„ç»„åˆæ ‡ç­¾åå‘æ˜ å°„å›åŸå§‹çš„ä¸“å®¶å’Œç±»åˆ«IDã€‚
    """
    # 1. ä»æ‰¹æ¬¡ä¸­æå–æ•°æ®å¹¶è·å–ç»´åº¦ä¿¡æ¯
    # å‡è®¾ expert_masks å·²ç»æ˜¯ float ç±»å‹çš„äºŒå€¼ (0/1) Tensor
    expert_masks = batch['expert_masks']
    device = expert_masks.device
    
    # åŠ¨æ€è·å–ç»´åº¦ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªé€‚åº”ä¸åŒæ•°æ®é›†
    B, N, C, H, W = expert_masks.shape
    
    # 2. ç”Ÿæˆæ–°çš„ç»„åˆç±»åˆ«æ ‡ç­¾
    # åˆ›å»ºä¸€ä¸ªä» 0 åˆ° N*C-1 çš„è¿ç»­æ•´æ•°ä½œä¸ºæ–°çš„æ ‡ç­¾
    # å½¢çŠ¶ä¸º (B, N*C)
    total_combined_classes = N * C
    new_labels = torch.arange(total_combined_classes, device=device).repeat(B, 1)
    print(f"new_labels: {new_labels}")

    # 3. é‡å¡‘æ©ç ä»¥åŒ¹é…æ–°æ ‡ç­¾
    # å°†ä¸“å®¶å’Œç±»åˆ«ç»´åº¦åˆå¹¶
    # [B, N, C, H, W] -> [B, N*C, H, W]
    reshaped_masks = expert_masks.view(B, total_combined_classes, H, W)

    # 4. æ„å»ºæŸå¤±å‡½æ•°æ‰€éœ€çš„ `targets` åˆ—è¡¨
    targets = []
    for i in range(B):
        targets.append({
            "labels": new_labels[i],
            "masks": reshaped_masks[i]
        })

    # 5. æ„å»ºç”¨äºåå‘æ˜ å°„å’Œé‡æ„çš„å…ƒæ•°æ®
    # åˆ›å»ºä¸€ä¸ªæ˜ å°„å¼ é‡ï¼Œå¤§å°ä¸º (N*C, 2)
    # mapping_tensor[new_label] = [expert_id, class_id]
    
    # [0, 0, ..., 1, 1, ..., N-1, N-1, ...] (æ¯ä¸ªé‡å¤Cæ¬¡)
    expert_ids_map = torch.arange(N, device=device).view(N, 1).repeat(1, C).view(-1)
    print(f"expert_ids_map: {expert_ids_map}")
    
    # [0, 1, ..., C-1, 0, 1, ..., C-1, ...] (é‡å¤Næ¬¡)
    class_ids_map = torch.arange(C, device=device).repeat(N)
    print(f"class_ids_map: {class_ids_map}")

    # å°†å®ƒä»¬å †å æˆ [N*C, 2] çš„æ˜ å°„å…³ç³»
    mapping_tensor = torch.stack([expert_ids_map, class_ids_map], dim=1)
    print(f"mapping_tensor: {mapping_tensor}")

    metadata = {
        'num_experts': N,
        'num_classes': C,
        'total_combined_classes': total_combined_classes,
        'mapping_tensor': mapping_tensor
    }

    return targets, metadata


# ==============================================================================
# å‘½ä»¤è¡Œå‚æ•°å¤„ç†
# ==============================================================================


def setup_training_args_parser():
    parser = argparse.ArgumentParser(description='Flexible EDL MaskFormer Training')
    
    # åŸºç¡€é…ç½®
    parser.add_argument('--config_path', type=str, required=True, help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--gpu', type=str, default='0', help='GPUè®¾å¤‡ID')
    
    # å®éªŒé…ç½®è¦†å†™
    parser.add_argument('--save_dir', type=str, help='è¦†å†™ä¿å­˜ç›®å½•')
    parser.add_argument('--max_checkpoints', type=int, help='è¦†å†™æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡')
    
    # æ•°æ®é›†é…ç½®è¦†å†™
    parser.add_argument('--dataset_config_path', type=str, help='è¦†å†™æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dataset_type', type=str, help='è¦†å†™æ•°æ®é›†ç±»å‹')
    
    # è®­ç»ƒé…ç½®è¦†å†™
    parser.add_argument('--seed', type=int, help='è¦†å†™éšæœºç§å­')
    parser.add_argument('--epochs', type=int, help='è¦†å†™è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, help='è¦†å†™æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--use_amp', type=bool, help='è¦†å†™æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦')
    
    # å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨é…ç½®è¦†å†™
    parser.add_argument('--learning_rate', '--lr', type=float, help='è¦†å†™å­¦ä¹ ç‡')
    parser.add_argument('--min_learning_rate', type=float, help='è¦†å†™æœ€å°å­¦ä¹ ç‡')
    parser.add_argument('--optimizer_type', type=str, choices=['AdamW', 'Adam', 'SGD'], help='è¦†å†™ä¼˜åŒ–å™¨ç±»å‹')
    parser.add_argument('--weight_decay', type=float, help='è¦†å†™æƒé‡è¡°å‡')
    parser.add_argument('--warmup_epochs', type=int, help='è¦†å†™é¢„çƒ­è½®æ•°')
    parser.add_argument('--use_cosine_scheduler', type=bool, help='è¦†å†™æ˜¯å¦ä½¿ç”¨ä½™å¼¦è°ƒåº¦å™¨')
    
    # å…¶ä»–
    parser.add_argument('--resume', type=str, help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒçš„è·¯å¾„')

    return parser


def apply_args_override(config, args):
    """åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†å†™YAMLé…ç½®ï¼Œæ”¯æŒåµŒå¥—é…ç½®ç»“æ„ã€‚"""
    override_params = {}
    original_values = {}
    
    # å®šä¹‰å‚æ•°åˆ°é…ç½®èŠ‚ç‚¹çš„æ˜ å°„
    param_mapping = {
        # å®éªŒé…ç½®
        'save_dir': ('experiment', 'save_dir'),
        'max_checkpoints': ('experiment', 'max_checkpoints'),
        
        # æ•°æ®é›†é…ç½®
        'dataset_config_path': ('dataset', 'config_path'),
        'dataset_type': ('dataset', 'type'),
        
        # è®­ç»ƒé…ç½®
        'seed': ('training', 'seed'),
        'epochs': ('training', 'epochs'),
        'batch_size': ('training', 'batch_size'),
        'use_amp': ('training', 'use_amp'),
        'learning_rate': ('training', 'learning_rate'),
        'min_learning_rate': ('training', 'min_learning_rate'),
        'optimizer_type': ('training', 'optimizer_type'),
        'weight_decay': ('training', 'weight_decay'),
        'warmup_epochs': ('training', 'warmup_epochs'),
        'use_cosine_scheduler': ('training', 'use_cosine_scheduler'),
    }
    
    for arg_name, arg_value in vars(args).items():
        if arg_value is not None:
            # å¤„ç†learning_rateçš„åˆ«å
            if arg_name == 'lr':
                arg_name = 'learning_rate'
            
            # è·³è¿‡éé…ç½®å‚æ•°
            if arg_name in ['config_path', 'gpu', 'resume']:
                continue
            
            # è·å–å‚æ•°æ˜ å°„
            if arg_name in param_mapping:
                section, key = param_mapping[arg_name]
                
                # ç¡®ä¿é…ç½®èŠ‚å­˜åœ¨
                if section not in config:
                    config[section] = {}
                
                # è®°å½•åŸå€¼å’Œè¦†å†™
                config_path = f'{section}.{key}'
                if key in config[section]:
                    original_values[config_path] = config[section][key]
                    if config[section][key] != arg_value:
                        override_params[config_path] = {
                            'original': config[section][key],
                            'override': arg_value
                        }
                
                # åº”ç”¨è¦†å†™
                config[section][key] = arg_value
            else:
                # å¤„ç†é¡¶çº§å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
                if arg_name in config and config[arg_name] != arg_value:
                    override_params[arg_name] = {
                        'original': config[arg_name],
                        'override': arg_value
                    }
                config[arg_name] = arg_value
    
    return config, override_params


def save_override_record(config, override_params, args, save_dir):
    """ä¿å­˜è¯¦ç»†çš„å‚æ•°è¦†å†™è®°å½•åˆ°æ–‡ä»¶ã€‚"""
    if not override_params:
        log_info("æ— å‚æ•°è¦†å†™ï¼Œè·³è¿‡è¦†å†™è®°å½•ä¿å­˜ã€‚", print_message=False)
        return
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)
    
    override_path = os.path.join(save_dir, 'args_override_record.txt')
    
    with open(override_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("EDLformer è®­ç»ƒå‚æ•°è¦†å†™è®°å½•\n")
        f.write("="*80 + "\n\n")
        
        f.write(f"è¦†å†™æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»è¦†å†™å‚æ•°æ•°é‡: {len(override_params)}\n\n")
        
        f.write("è¦†å†™è¯¦æƒ…:\n")
        f.write("-" * 60 + "\n")
        
        for config_path, change_info in override_params.items():
            original_val = change_info['original']
            override_val = change_info['override']
            f.write(f"å‚æ•°è·¯å¾„: {config_path}\n")
            f.write(f"  åŸå§‹å€¼: {original_val} ({type(original_val).__name__})\n")
            f.write(f"  è¦†å†™å€¼: {override_val} ({type(override_val).__name__})\n")
            f.write(f"  å˜æ›´: {original_val} -> {override_val}\n")
            f.write("-" * 40 + "\n")
        
        f.write("\n" + "="*80 + "\n")
        f.write("å®Œæ•´çš„å‘½ä»¤è¡Œå‚æ•°:\n")
        f.write("="*80 + "\n")
        for arg_name, arg_value in vars(args).items():
            if arg_value is not None:
                f.write(f"--{arg_name}: {arg_value}\n")
        
        f.write("\n" + "="*80 + "\n")
        f.write("è¦†å†™åçš„å®Œæ•´é…ç½®æ‘˜è¦:\n")
        f.write("="*80 + "\n")
        
        # è¾“å‡ºå…³é”®é…ç½®ä¿¡æ¯
        training_cfg = config.get('training', {})
        f.write("è®­ç»ƒé…ç½®:\n")
        for key in ['seed', 'epochs', 'batch_size', 'learning_rate', 'optimizer_type']:
            if key in training_cfg:
                f.write(f"  {key}: {training_cfg[key]}\n")
        
        dataset_cfg = config.get('dataset', {})
        if dataset_cfg:
            f.write("æ•°æ®é›†é…ç½®:\n")
            for key, value in dataset_cfg.items():
                f.write(f"  {key}: {value}\n")
        
        experiment_cfg = config.get('experiment', {})
        if experiment_cfg:
            f.write("å®éªŒé…ç½®:\n")
            for key, value in experiment_cfg.items():
                f.write(f"  {key}: {value}\n")
    
    log_info(f"å‚æ•°è¦†å†™è®°å½•å·²ä¿å­˜åˆ°: {override_path}", print_message=True)


def identify_overridden_params(args):
    """è¯†åˆ«å“ªäº›å‚æ•°é€šè¿‡å‘½ä»¤è¡Œè¿›è¡Œäº†è¦†å†™ã€‚"""
    overridden_params = []
    
    for arg_name, arg_value in vars(args).items():
        if arg_value is not None and arg_name not in ['config_path', 'gpu', 'resume']:
            overridden_params.append(arg_name)
    
    return overridden_params


def print_override_summary(override_params):
    """æ‰“å°å‚æ•°è¦†å†™æ‘˜è¦ã€‚"""
    if not override_params:
        log_info("âœ… æœªæ£€æµ‹åˆ°å‚æ•°è¦†å†™ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼ã€‚", print_message=True)
        return
    
    log_info("="*60, print_message=True)
    log_info(f"ğŸ”§ æ£€æµ‹åˆ° {len(override_params)} ä¸ªå‚æ•°è¦†å†™:", print_message=True)
    log_info("="*60, print_message=True)
    
    for config_path, change_info in override_params.items():
        original_val = change_info['original']
        override_val = change_info['override']
        log_info(f"ğŸ“ {config_path}: {original_val} â†’ {override_val}", print_message=True)
    
    log_info("="*60, print_message=True)
            

# ==============================================================================
# æ¨¡å‹ä¿å­˜ä¸åŠ è½½ (Checkpointing)
# ==============================================================================


def save_checkpoint(state, is_best, save_dir, epoch, last_checkpoints: deque, max_checkpoints=3):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚"""
    filename = os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth')
    torch.save(state, filename)
    last_checkpoints.append(filename)
    if is_best:
        best_path = os.path.join(save_dir, 'best_model.pth')
        shutil.copyfile(filename, best_path)
    while len(last_checkpoints) > max_checkpoints:
        old_ckpt = last_checkpoints.popleft()
        if os.path.exists(old_ckpt):
            os.remove(old_ckpt)
    log_info(f"å·²ä¿å­˜æ£€æŸ¥ç‚¹: {filename}", print_message=False)


# ==============================================================================
# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
# ==============================================================================
if __name__ == '__main__':
    
    def simulate_and_test(B, N, C, H, W):
        print(f"\n--- æµ‹è¯•æ•°æ®é›†é…ç½®: B={B}, N={N}, C={C} ---")
        
        # 1. æ¨¡æ‹Ÿä¸€ä¸ªæ¥è‡ª RIGADatasetSimpleV2 çš„ batch
        #    æ¯ä¸ªæ©ç éƒ½æ˜¯éšæœºçš„äºŒå€¼å›¾åƒ
        mock_expert_masks = torch.randint(0, 2, (B, N, C, H, W), dtype=torch.float32).cuda()
        mock_batch = {"expert_masks": mock_expert_masks}
        
        # 2. è°ƒç”¨å¤„ç†å‡½æ•°
        targets, metadata = process_batch_for_expert_class_combination(mock_batch)
        
        # 3. éªŒè¯è¾“å‡º
        print("å…ƒæ•°æ® (metadata):")
        print(f"  - ä¸“å®¶æ•° (N): {metadata['num_experts']}")
        print(f"  - åŸå§‹ç±»åˆ«æ•° (C): {metadata['num_classes']}")
        print(f"  - ç»„åˆåæ€»ç±»åˆ«æ•° (N*C): {metadata['total_combined_classes']}")
        print(f"  - æ˜ å°„å¼ é‡å½¢çŠ¶: {metadata['mapping_tensor'].shape}")
        
        # æ‰“å°éƒ¨åˆ†æ˜ å°„å…³ç³»ä»¥ä¾›æ£€æŸ¥
        print("\néƒ¨åˆ†æ˜ å°„å…³ç³» (new_label -> [expert_id, class_id]):")
        for i in list(range(min(5, N*C))) + list(range(max(0, N*C-5), N*C)):
             new_label = i
             expert_id = metadata['mapping_tensor'][new_label, 0].item()
             class_id = metadata['mapping_tensor'][new_label, 1].item()
             print(f"  - ç»„åˆæ ‡ç­¾ {new_label:2d} -> ä¸“å®¶ {expert_id}, ç±»åˆ« {class_id}")

        print("\nTargets æ ¼å¼æ£€æŸ¥:")
        print(f"  - `targets` åˆ—è¡¨é•¿åº¦: {len(targets)} (åº”ä¸º B={B})")
        assert len(targets) == B
        
        first_sample_target = targets[0]
        print(f"  - ç¬¬ä¸€ä¸ªæ ·æœ¬ 'labels' å½¢çŠ¶: {first_sample_target['labels'].shape} (åº”ä¸º [{N*C}])")
        assert first_sample_target['labels'].shape[0] == N * C
        
        print(f"  - ç¬¬ä¸€ä¸ªæ ·æœ¬ 'masks' å½¢çŠ¶: {first_sample_target['masks'].shape} (åº”ä¸º [{N*C}, {H}, {W}])")
        assert first_sample_target['masks'].shape == (N * C, H, W)
        
        # éªŒè¯æ•°æ®å†…å®¹æ˜¯å¦ä¸€è‡´
        original_mask_sample = mock_batch['expert_masks'][0] # [N, C, H, W]
        processed_mask_sample = first_sample_target['masks'].view(N, C, H, W)
        assert torch.equal(original_mask_sample, processed_mask_sample)
        print("  - æ•°æ®å†…å®¹ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡!")
        print("-" * (40))

    # --- è¿è¡Œä¸åŒé…ç½®çš„æµ‹è¯• ---
    # RIGA æ•°æ®é›†æƒ…å†µ
    simulate_and_test(B=4, N=6, C=2, H=64, W=64)
    
    # å…¶ä»–å¯èƒ½çš„æ•°æ®é›†æƒ…å†µ
    simulate_and_test(B=8, N=4, C=1, H=32, W=32)
    simulate_and_test(B=2, N=4, C=3, H=48, W=48)
    
    print("\n" + "="*80)
    print("ç»Ÿä¸€è®­ç»ƒé…ç½®ä½¿ç”¨ç¤ºä¾‹:")
    print("="*80)
    
    # ç¤ºä¾‹é…ç½®
    example_config = {
        'epochs': 100,
        'batch_size': 8,
        'dataset_yaml': './codes/configs/RIGA.yaml',
        'model': {
            'num_cls_classes': 12,
            'num_queries': 12,
            'in_channels': 3
        },
        'loss': {
            'num_classes': 2
        },
        'training': {
            'learning_rate': 1e-3,
            'optimizer_type': 'AdamW',
            'weight_decay': 1e-4,
            'use_cosine_scheduler': True,
            'warmup_epochs': 10
        }
    }
    
    print("é…ç½®ç¤ºä¾‹:")
    print("training:")
    print("  learning_rate: 1e-3      # ç»Ÿä¸€å­¦ä¹ ç‡å£°æ˜")
    print("  optimizer_type: 'AdamW'  # ä¼˜åŒ–å™¨ç±»å‹")
    print("  weight_decay: 1e-4       # æƒé‡è¡°å‡")
    print("  use_cosine_scheduler: true")
    print("  warmup_epochs: 10")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("model, loss_fn, train_loader, val_loader, optimizer, scheduler = build_complete_training_setup(config)")
    print("="*80)