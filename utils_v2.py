import os
import sys
import yaml
import torch
import random
import numpy as np
import datetime
from typing import List, Dict, Tuple, Any
from torch.utils.data import DataLoader
from collections import deque
import shutil
import json
import argparse

# --- [æ ¸å¿ƒä¿®æ”¹] å¯¼å…¥æ‰€æœ‰éœ€è¦çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬æ–°çš„ç»Ÿä¸€æ¨¡å‹å’Œæ—§æ¨¡å‹ ---
# from nets.simple_maskformer import SimpleMaskFormer, SimpleMaskFormerMulti
from nets.simple_edl_maskformer import FlexibleEDLMaskFormer

# --- [æ ¸å¿ƒä¿®æ”¹] å¯¼å…¥ä¸¤ç§æ¨¡å¼å¯¹åº”çš„EDLæŸå¤±å‡½æ•° ---
# from scheduler.edl_single_loss import EDLSingleLoss
# from scheduler.simple_maskformer_loss import SimpleMaskformerLoss
# from scheduler.simple_maskformer_multi_loss import SimpleMaskFormerMultiLoss
from scheduler.simple_edl_single_maskformer_loss import SimpleEDLMaskformerLossV2

# å¯¼å…¥æ•°æ®é›†å’Œå…¶ä»–è¾…åŠ©æ¨¡å—
from datasets import RIGADatasetSimpleV2
from logger import log_info, log_error
from torch.utils.tensorboard import SummaryWriter

# ==============================================================================
# åŸºç¡€è®¾ç½® (Setup)
# ==============================================================================

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒçš„å¯é‡å¤æ€§ã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    log_info(f"éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}", print_message=False)

def load_config(yaml_path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶ã€‚"""
    with open(yaml_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

# ==============================================================================
# [æ ¸å¿ƒä¿®æ”¹] ç»Ÿä¸€çš„ç»„ä»¶æ„å»ºå™¨
# ==============================================================================

def build_edl_components(config):
    """æ ¹æ®é…ç½®æ„å»ºæ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®é›†ï¼Œå¹¶è¿”å›å…ƒæ•°æ®ã€‚"""
    model_config = config['model']
    loss_config = config.get('loss', {})
    training_config = config.get('training', {})
    dataset_config = config.get('dataset', {})
    dataset_yaml = dataset_config.get('config_path')
    # ä¿®å¤ç›¸å¯¹è·¯å¾„é—®é¢˜
    if dataset_yaml and not os.path.isabs(dataset_yaml):
        dataset_yaml = os.path.join('/app/MultiAnn/EDLformer', dataset_yaml.lstrip('./'))
    if not dataset_yaml or not os.path.exists(dataset_yaml):
        raise FileNotFoundError(f"æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {dataset_yaml}")
    train_dataset = RIGADatasetSimpleV2(config_path=dataset_yaml, is_train=True)
    val_dataset = RIGADatasetSimpleV2(config_path=dataset_yaml, is_train=False)
    train_loader = DataLoader(train_dataset, batch_size=training_config.get('batch_size', 8), shuffle=True, num_workers=training_config.get('num_workers', 4), pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=training_config.get('num_workers', 4), pin_memory=True)
    sample_data = train_dataset[0]
    expert_masks_shape = sample_data["expert_masks"].shape
    log_info(f"æ•°æ®é›†expert_maskså½¢çŠ¶: {expert_masks_shape}", print_message=True)
    
    if len(expert_masks_shape) == 4:
        # å½¢çŠ¶: (num_experts, num_seg_classes, H, W)
        num_experts, num_seg_classes, _, _ = expert_masks_shape
    elif len(expert_masks_shape) == 5:
        # å½¢çŠ¶: (1, num_experts, num_seg_classes, H, W)
        _, num_experts, num_seg_classes, _, _ = expert_masks_shape
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„expert_maskså½¢çŠ¶: {expert_masks_shape}")
    total_combined_classes = num_experts * num_seg_classes
    dataset_metadata = {'num_experts': num_experts, 'num_seg_classes': num_seg_classes, 'total_combined_classes': total_combined_classes}
    log_info(f"æ•°æ®é›†æ„å»ºå®Œæˆã€‚åŠ¨æ€æ¨æ–­: {num_experts}ä¸“å®¶, {num_seg_classes}åˆ†å‰²ç±»åˆ«, {total_combined_classes}ç»„åˆç±»åˆ«ã€‚", print_message=True)
    if model_config.get('num_cls_classes') != total_combined_classes:
        log_info(f"è­¦å‘Š: æ¨¡å‹é…ç½®'num_cls_classes'({model_config.get('num_cls_classes')})ä¸æ¨æ–­å€¼({total_combined_classes})ä¸åŒ¹é…ã€‚", print_message=True)
    model = FlexibleEDLMaskFormer(**model_config)
    log_info("æ¨¡å‹æ„å»ºå®Œæˆã€‚", print_message=True)
    loss_params = {k: v for k, v in loss_config.items()}
    loss_params['num_cls_classes'] = total_combined_classes
    loss_fn = SimpleEDLMaskformerLossV2(**loss_params)
    log_info("æŸå¤±å‡½æ•°æ„å»ºå®Œæˆã€‚", print_message=True)
    return model.cuda(), loss_fn, train_loader, val_loader, dataset_metadata

def build_complete_training_setup(config, args):
    """æ„å»ºå®Œæ•´çš„è®­ç»ƒè®¾ç½®ï¼Œå¹¶åº”ç”¨å‚æ•°è¦†å†™ã€‚"""
    log_info("="*80, print_message=True)
    log_info("å¼€å§‹æ„å»ºå®Œæ•´è®­ç»ƒè®¾ç½®...", print_message=True)
    
    # [å…³é”®æ”¹åŠ¨] åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†å†™
    config, override_params = apply_args_override(config, args)
    if override_params:
        print_override_summary(override_params)
    
    model, loss_fn, train_loader, val_loader, dataset_metadata = build_edl_components(config)
    optimizer, scheduler = build_optimizer_and_scheduler(model, config)
    
    log_info("å®Œæ•´è®­ç»ƒè®¾ç½®æ„å»ºå®Œæˆã€‚", print_message=True)
    log_info(f"ğŸ¯ å®éªŒä¿å­˜ç›®å½•: {config.get('experiment', {}).get('save_dir', 'N/A')}", print_message=True)
    log_info("="*80, print_message=True)
    
    # [å…³é”®æ”¹åŠ¨] è¿”å›è¦†å†™è®°å½•ï¼Œä¾›ä¸»å‡½æ•°ä¿å­˜
    return model, loss_fn, train_loader, val_loader, optimizer, scheduler, dataset_metadata, config, override_params



def build_optimizer_and_scheduler(model, config):
    """ç»Ÿä¸€æ„å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œä½¿ç”¨å•ä¸€å­¦ä¹ ç‡é…ç½®ã€‚"""
    training_cfg = config.get('training', {})
    
    # ç»Ÿä¸€å­¦ä¹ ç‡é…ç½® - å¼ºåˆ¶ä½¿ç”¨å•ä¸ªå­¦ä¹ ç‡å£°æ˜
    base_lr = float(training_cfg.get('learning_rate', 1e-3))
    log_info(f"ä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡é…ç½®: {base_lr}", print_message=True)
    
    # ä¼˜åŒ–å™¨é…ç½®
    optimizer_type = training_cfg.get('optimizer_type', 'AdamW')
    weight_decay = float(training_cfg.get('weight_decay', 1e-4))
    
    # æ„å»ºä¼˜åŒ–å™¨
    if optimizer_type.lower() == 'adamw':
        betas = training_cfg.get('betas', [0.9, 0.999])
        if isinstance(betas, list) and len(betas) == 2:
            betas = (float(betas[0]), float(betas[1]))
        
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=base_lr,
            weight_decay=weight_decay,
            betas=betas,
            eps=float(training_cfg.get('eps', 1e-8)),
            amsgrad=bool(training_cfg.get('amsgrad', False))
        )
        log_info(f"æ„å»ºAdamWä¼˜åŒ–å™¨: lr={base_lr}, weight_decay={weight_decay}", print_message=False)
        
    elif optimizer_type.lower() == 'adam':
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=base_lr,
            weight_decay=weight_decay
        )
        log_info(f"æ„å»ºAdamä¼˜åŒ–å™¨: lr={base_lr}, weight_decay={weight_decay}", print_message=False)
        
    elif optimizer_type.lower() == 'sgd':
        momentum = float(training_cfg.get('momentum', 0.9))
        optimizer = torch.optim.SGD(
            model.parameters(),
            lr=base_lr,
            weight_decay=weight_decay,
            momentum=momentum
        )
        log_info(f"æ„å»ºSGDä¼˜åŒ–å™¨: lr={base_lr}, weight_decay={weight_decay}, momentum={momentum}", print_message=False)
    else:
        raise ValueError(f"æœªçŸ¥çš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
    use_scheduler = training_cfg.get('use_cosine_scheduler', False)
    if not use_scheduler:
        log_info("ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚", print_message=False)
        return optimizer, None
    
    # æ„å»ºä½™å¼¦é€€ç«è°ƒåº¦å™¨
    # ä¼˜å…ˆä» training èŠ‚ç‚¹è·å– epochsï¼Œå¦åˆ™ä»é¡¶çº§èŠ‚ç‚¹è·å–
    total_epochs = training_cfg.get('epochs', config.get('epochs', 100))
    warmup_epochs = int(training_cfg.get('warmup_epochs', 0))
    min_lr = float(training_cfg.get('min_learning_rate', 1e-6))
    
    def lr_lambda(epoch):
        if warmup_epochs > 0 and epoch < warmup_epochs:
            # çº¿æ€§warmup
            return float(epoch + 1) / float(max(1, warmup_epochs))
        
        # ä½™å¼¦é€€ç«
        progress = float(epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))
        progress = min(max(progress, 0.0), 1.0)
        cosine = 0.5 * (1. + np.cos(np.pi * progress))
        return cosine * (1.0 - min_lr / base_lr) + min_lr / base_lr
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    log_info(f"æ„å»ºä½™å¼¦é€€ç«è°ƒåº¦å™¨: warmup_epochs={warmup_epochs}, min_lr={min_lr}", print_message=False)
    
    return optimizer, scheduler


def process_batch_for_expert_class_combination(
    batch: Dict[str, torch.Tensor]
) -> Tuple[List[Dict[str, torch.Tensor]], Dict[str, Any]]:
    """
    å¤„ç†æ¥è‡ªå¤šä¸“å®¶æ ‡æ³¨æ•°æ®é›†çš„æ‰¹æ¬¡æ•°æ®ï¼Œå°†(ä¸“å®¶, ç±»åˆ«)ç»„åˆè§†ä¸ºæ–°ç±»åˆ«ã€‚

    æ­¤å‡½æ•°æ—¨åœ¨ä¸ºç±» MaskFormer æ¨¡å‹å‡†å¤‡ 'targets'ï¼ŒåŒæ—¶ç”Ÿæˆç”¨äºç»“æœé‡æ„çš„å…ƒæ•°æ®ã€‚

    Args:
        batch (Dict[str, torch.Tensor]): 
            ä» DataLoader è¾“å‡ºçš„æ‰¹æ¬¡å­—å…¸ï¼Œè‡³å°‘éœ€è¦åŒ…å«:
            - 'expert_masks': torch.Tensor, å½¢çŠ¶ä¸º [B, N, C, H, W]
              B: æ‰¹æ¬¡å¤§å°, N: ä¸“å®¶æ•°, C: åŸå§‹ç±»åˆ«æ•°, H, W: æ©ç é«˜å®½ã€‚
              æ©ç åº”ä¸ºäºŒå€¼ (0 æˆ– 1)ã€‚

    Returns:
        Tuple[List[Dict[str, torch.Tensor]], Dict[str, Any]]:
        - targets (List[Dict[str, torch.Tensor]]):
            ä¸€ä¸ªåˆ—è¡¨ï¼Œé•¿åº¦ä¸ºBã€‚æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«:
            - 'labels': 1D Tensor, å½¢çŠ¶ä¸º [N*C]ï¼ŒåŒ…å«æ–°ç”Ÿæˆçš„å”¯ä¸€ç±»åˆ«IDã€‚
            - 'masks': 3D Tensor, å½¢çŠ¶ä¸º [N*C, H, W]ï¼ŒåŒ…å«å¯¹åº”çš„äºŒå€¼æ©ç ã€‚
            æ­¤æ ¼å¼å¯ç›´æ¥ç”¨äº DETR/MaskFormer ç±»çš„æŸå¤±å‡½æ•°ã€‚

        - metadata (Dict[str, Any]):
            ä¸€ä¸ªå…ƒæ•°æ®å­—å…¸ï¼Œç”¨äºåç»­ç»“æœè§£æå’Œé‡æ„ï¼ŒåŒ…å«:
            - 'num_experts' (int): åŸå§‹ä¸“å®¶æ•° Nã€‚
            - 'num_classes' (int): åŸå§‹ç±»åˆ«æ•° Cã€‚
            - 'total_combined_classes' (int): N * Cï¼Œç»„åˆåçš„æ€»ç±»åˆ«æ•°ã€‚
            - 'mapping_tensor': 2D Tensor, å½¢çŠ¶ä¸º [N*C, 2]ã€‚
              æ¯ä¸€è¡Œ `mapping_tensor[new_label]` åŒ…å«äº† `[expert_id, class_id]`ï¼Œ
              å¯ä»¥å°†æ–°çš„ç»„åˆæ ‡ç­¾åå‘æ˜ å°„å›åŸå§‹çš„ä¸“å®¶å’Œç±»åˆ«IDã€‚
    """
    # 1. ä»æ‰¹æ¬¡ä¸­æå–æ•°æ®å¹¶è·å–ç»´åº¦ä¿¡æ¯
    # å‡è®¾ expert_masks å·²ç»æ˜¯ float ç±»å‹çš„äºŒå€¼ (0/1) Tensor
    expert_masks = batch['expert_masks']
    device = expert_masks.device
    
    # åŠ¨æ€è·å–ç»´åº¦ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªé€‚åº”ä¸åŒæ•°æ®é›†
    B, N, C, H, W = expert_masks.shape
    
    # 2. ç”Ÿæˆæ–°çš„ç»„åˆç±»åˆ«æ ‡ç­¾
    # åˆ›å»ºä¸€ä¸ªä» 0 åˆ° N*C-1 çš„è¿ç»­æ•´æ•°ä½œä¸ºæ–°çš„æ ‡ç­¾
    # å½¢çŠ¶ä¸º (B, N*C)
    total_combined_classes = N * C
    new_labels = torch.arange(total_combined_classes, device=device).repeat(B, 1)
    # print(f"new_labels: {new_labels}")

    # 3. é‡å¡‘æ©ç ä»¥åŒ¹é…æ–°æ ‡ç­¾
    # å°†ä¸“å®¶å’Œç±»åˆ«ç»´åº¦åˆå¹¶
    # [B, N, C, H, W] -> [B, N*C, H, W]
    reshaped_masks = expert_masks.view(B, total_combined_classes, H, W)

    # 4. æ„å»ºæŸå¤±å‡½æ•°æ‰€éœ€çš„ `targets` åˆ—è¡¨
    targets = []
    for i in range(B):
        targets.append({
            "labels": new_labels[i],
            "masks": reshaped_masks[i]
        })

    # 5. æ„å»ºç”¨äºåå‘æ˜ å°„å’Œé‡æ„çš„å…ƒæ•°æ®
    # åˆ›å»ºä¸€ä¸ªæ˜ å°„å¼ é‡ï¼Œå¤§å°ä¸º (N*C, 2)
    # mapping_tensor[new_label] = [expert_id, class_id]
    
    # [0, 0, ..., 1, 1, ..., N-1, N-1, ...] (æ¯ä¸ªé‡å¤Cæ¬¡)
    expert_ids_map = torch.arange(N, device=device).view(N, 1).repeat(1, C).view(-1)
    # print(f"expert_ids_map: {expert_ids_map}")
    
    # [0, 1, ..., C-1, 0, 1, ..., C-1, ...] (é‡å¤Næ¬¡)
    class_ids_map = torch.arange(C, device=device).repeat(N)
    # print(f"class_ids_map: {class_ids_map}")

    # å°†å®ƒä»¬å †å æˆ [N*C, 2] çš„æ˜ å°„å…³ç³»
    mapping_tensor = torch.stack([expert_ids_map, class_ids_map], dim=1)
    # print(f"mapping_tensor: {mapping_tensor}")

    metadata = {
        'num_experts': N,
        'num_classes': C,
        'total_combined_classes': total_combined_classes,
        'mapping_tensor': mapping_tensor
    }

    return targets, metadata


# ==============================================================================
# å‘½ä»¤è¡Œå‚æ•°å¤„ç†
# ==============================================================================


def setup_training_args_parser():
    parser = argparse.ArgumentParser(description='Flexible EDL MaskFormer Training')
    
    # åŸºç¡€é…ç½®
    parser.add_argument('--config_path', type=str, required=True, help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--gpu', type=str, default='0', help='GPUè®¾å¤‡ID')
    
    # å®éªŒé…ç½®è¦†å†™
    parser.add_argument('--save_dir', type=str, help='è¦†å†™ä¿å­˜ç›®å½•')
    parser.add_argument('--max_checkpoints', type=int, help='è¦†å†™æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡')
    
    # æ•°æ®é›†é…ç½®è¦†å†™
    parser.add_argument('--dataset_config_path', type=str, help='è¦†å†™æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dataset_type', type=str, help='è¦†å†™æ•°æ®é›†ç±»å‹')
    
    # è®­ç»ƒé…ç½®è¦†å†™
    parser.add_argument('--seed', type=int, help='è¦†å†™éšæœºç§å­')
    parser.add_argument('--epochs', type=int, help='è¦†å†™è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, help='è¦†å†™æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--use_amp', type=bool, help='è¦†å†™æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦')
    
    # å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨é…ç½®è¦†å†™
    parser.add_argument('--learning_rate', '--lr', type=float, help='è¦†å†™å­¦ä¹ ç‡')
    parser.add_argument('--min_learning_rate', type=float, help='è¦†å†™æœ€å°å­¦ä¹ ç‡')
    parser.add_argument('--optimizer_type', type=str, choices=['AdamW', 'Adam', 'SGD'], help='è¦†å†™ä¼˜åŒ–å™¨ç±»å‹')
    parser.add_argument('--weight_decay', type=float, help='è¦†å†™æƒé‡è¡°å‡')
    parser.add_argument('--warmup_epochs', type=int, help='è¦†å†™é¢„çƒ­è½®æ•°')
    parser.add_argument('--use_cosine_scheduler', type=bool, help='è¦†å†™æ˜¯å¦ä½¿ç”¨ä½™å¼¦è°ƒåº¦å™¨')
    
    # å…¶ä»–
    parser.add_argument('--resume', type=str, help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒçš„è·¯å¾„')

    return parser


def apply_args_override(config, args):
    """åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†å†™YAMLé…ç½®ï¼Œæ”¯æŒåµŒå¥—é…ç½®ç»“æ„ã€‚"""
    override_params = {}
    original_values = {}
    
    # å®šä¹‰å‚æ•°åˆ°é…ç½®èŠ‚ç‚¹çš„æ˜ å°„
    param_mapping = {
        # å®éªŒé…ç½®
        'save_dir': ('experiment', 'save_dir'),
        'max_checkpoints': ('experiment', 'max_checkpoints'),
        
        # æ•°æ®é›†é…ç½®
        'dataset_config_path': ('dataset', 'config_path'),
        'dataset_type': ('dataset', 'type'),
        
        # è®­ç»ƒé…ç½®
        'seed': ('training', 'seed'),
        'epochs': ('training', 'epochs'),
        'batch_size': ('training', 'batch_size'),
        'use_amp': ('training', 'use_amp'),
        'learning_rate': ('training', 'learning_rate'),
        'min_learning_rate': ('training', 'min_learning_rate'),
        'optimizer_type': ('training', 'optimizer_type'),
        'weight_decay': ('training', 'weight_decay'),
        'warmup_epochs': ('training', 'warmup_epochs'),
        'use_cosine_scheduler': ('training', 'use_cosine_scheduler'),
    }
    
    for arg_name, arg_value in vars(args).items():
        if arg_value is not None:
            # å¤„ç†learning_rateçš„åˆ«å
            if arg_name == 'lr':
                arg_name = 'learning_rate'
            
            # è·³è¿‡éé…ç½®å‚æ•°
            if arg_name in ['config_path', 'gpu', 'resume']:
                continue
            
            # è·å–å‚æ•°æ˜ å°„
            if arg_name in param_mapping:
                section, key = param_mapping[arg_name]
                
                # ç¡®ä¿é…ç½®èŠ‚å­˜åœ¨
                if section not in config:
                    config[section] = {}
                
                # è®°å½•åŸå€¼å’Œè¦†å†™
                config_path = f'{section}.{key}'
                if key in config[section]:
                    original_values[config_path] = config[section][key]
                    if config[section][key] != arg_value:
                        override_params[config_path] = {
                            'original': config[section][key],
                            'override': arg_value
                        }
                
                # åº”ç”¨è¦†å†™
                config[section][key] = arg_value
            else:
                # å¤„ç†é¡¶çº§å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
                if arg_name in config and config[arg_name] != arg_value:
                    override_params[arg_name] = {
                        'original': config[arg_name],
                        'override': arg_value
                    }
                config[arg_name] = arg_value
    
    return config, override_params

def save_override_record(config, override_params, args, save_dir):
    """ä¿å­˜è¯¦ç»†çš„å‚æ•°è¦†å†™è®°å½•åˆ°æ–‡ä»¶ã€‚"""
    if not override_params:
        log_info("æ— å‚æ•°è¦†å†™ï¼Œè·³è¿‡è¦†å†™è®°å½•ä¿å­˜ã€‚", print_message=False)
        return
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)
    
    override_path = os.path.join(save_dir, 'args_override_record.txt')
    
    with open(override_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("EDLformer è®­ç»ƒå‚æ•°è¦†å†™è®°å½•\n")
        f.write("="*80 + "\n\n")
        
        f.write(f"è¦†å†™æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»è¦†å†™å‚æ•°æ•°é‡: {len(override_params)}\n\n")
        
        f.write("è¦†å†™è¯¦æƒ…:\n")
        f.write("-" * 60 + "\n")
        
        for config_path, change_info in override_params.items():
            original_val = change_info['original']
            override_val = change_info['override']
            f.write(f"å‚æ•°è·¯å¾„: {config_path}\n")
            f.write(f"  åŸå§‹å€¼: {original_val} ({type(original_val).__name__})\n")
            f.write(f"  è¦†å†™å€¼: {override_val} ({type(override_val).__name__})\n")
            f.write(f"  å˜æ›´: {original_val} -> {override_val}\n")
            f.write("-" * 40 + "\n")
        
        f.write("\n" + "="*80 + "\n")
        f.write("å®Œæ•´çš„å‘½ä»¤è¡Œå‚æ•°:\n")
        f.write("="*80 + "\n")
        for arg_name, arg_value in vars(args).items():
            if arg_value is not None:
                f.write(f"--{arg_name}: {arg_value}\n")
        
        f.write("\n" + "="*80 + "\n")
        f.write("è¦†å†™åçš„å®Œæ•´é…ç½®æ‘˜è¦:\n")
        f.write("="*80 + "\n")
        
        # è¾“å‡ºå…³é”®é…ç½®ä¿¡æ¯
        training_cfg = config.get('training', {})
        f.write("è®­ç»ƒé…ç½®:\n")
        for key in ['seed', 'epochs', 'batch_size', 'learning_rate', 'optimizer_type']:
            if key in training_cfg:
                f.write(f"  {key}: {training_cfg[key]}\n")
        
        dataset_cfg = config.get('dataset', {})
        if dataset_cfg:
            f.write("æ•°æ®é›†é…ç½®:\n")
            for key, value in dataset_cfg.items():
                f.write(f"  {key}: {value}\n")
        
        experiment_cfg = config.get('experiment', {})
        if experiment_cfg:
            f.write("å®éªŒé…ç½®:\n")
            for key, value in experiment_cfg.items():
                f.write(f"  {key}: {value}\n")
    
    log_info(f"å‚æ•°è¦†å†™è®°å½•å·²ä¿å­˜åˆ°: {override_path}", print_message=True)

def print_override_summary(override_params):
    """åœ¨æ§åˆ¶å°æ‰“å°å‚æ•°è¦†å†™æ‘˜è¦ã€‚"""
    log_info("="*60, print_message=True)
    log_info(f"ğŸ”§ æ£€æµ‹åˆ° {len(override_params)} ä¸ªå‚æ•°è¦†å†™:", print_message=True)
    for config_path, change_info in override_params.items():
        log_info(f"  - {config_path}: {change_info['original']} â†’ {change_info['override']}", print_message=True)
    log_info("="*60, print_message=True)
            

# ==============================================================================
# æ¨¡å‹ä¿å­˜ä¸åŠ è½½ (Checkpointing)
# ==============================================================================


def save_checkpoint(state, is_best, save_dir, epoch, last_checkpoints: deque, max_checkpoints=3):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚"""
    filename = os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth')
    torch.save(state, filename)
    last_checkpoints.append(filename)
    if is_best:
        best_path = os.path.join(save_dir, 'best_model.pth')
        shutil.copyfile(filename, best_path)
    while len(last_checkpoints) > max_checkpoints:
        old_ckpt = last_checkpoints.popleft()
        if os.path.exists(old_ckpt):
            os.remove(old_ckpt)
    log_info(f"å·²ä¿å­˜æ£€æŸ¥ç‚¹: {filename}", print_message=False)


def load_checkpoint(checkpoint_path, model, optimizer=None, scaler=None):
    """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚"""
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    log_info(f"æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}", print_message=True)
    checkpoint = torch.load(checkpoint_path, map_location='cuda')
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    model.load_state_dict(checkpoint['state_dict'])
    
    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if optimizer is not None and 'optimizer' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer'])
        log_info("ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½", print_message=True)
    
    # åŠ è½½æ¿€æ´»å‡½æ•°ç¼©æ”¾å™¨çŠ¶æ€
    if scaler is not None and 'scaler' in checkpoint and checkpoint['scaler'] is not None:
        scaler.load_state_dict(checkpoint['scaler'])
        log_info("AMP ScalerçŠ¶æ€å·²åŠ è½½", print_message=True)
    
    start_epoch = checkpoint.get('epoch', 0)
    best_metric_score = checkpoint.get('best_metric_score', -np.inf)
    
    log_info(f"æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ: epoch={start_epoch}, best_score={best_metric_score:.6f}", print_message=True)
    
    return start_epoch, best_metric_score



# ==============================================================================
# TensorBoard æ—¥å¿—è®°å½•å™¨
# ==============================================================================

class TensorboardLogger:
    """ä¸€ä¸ªå°è£…äº†TensorBoard SummaryWriterçš„ç±»ï¼Œç®€åŒ–æ—¥å¿—è®°å½•ã€‚"""
    def __init__(self, log_dir):
        self.writer = SummaryWriter(log_dir=log_dir)

    def log_training_losses(self, loss_dict, global_step):
        """è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰€æœ‰æŸå¤±åˆ†é‡ã€‚"""
        for loss_name, loss_value in loss_dict.items():
            self.writer.add_scalar(f'Loss/{loss_name}', loss_value, global_step)

    def log_validation_metrics(self, metrics, train_losses, learning_rate, epoch):
        """è®°å½•éªŒè¯æŒ‡æ ‡ã€å¹³å‡è®­ç»ƒæŸå¤±å’Œå­¦ä¹ ç‡ã€‚"""
        # è®°å½•å¹³å‡è®­ç»ƒæŸå¤±ï¼ˆæ‰€æœ‰åˆ†é‡ï¼‰
        for loss_name, loss_value in train_losses.items():
            self.writer.add_scalar(f'Loss/train_avg_{loss_name}', loss_value, epoch)

        # è®°å½•è½¯ Dice åˆ†æ•°ï¼ˆæŒ‰ç±»åˆ«ï¼‰
        soft_dice = metrics.get('soft_dice', {})
        for metric_name, value in soft_dice.items():
            self.writer.add_scalar(f'Metrics/soft_dice_{metric_name}', value, epoch)
        
        # è®°å½•ä¸ªæ€§åŒ–æŒ‡æ ‡ - dice_per_expert
        dice_per_expert = metrics.get('dice_per_expert', {})
        for class_name, expert_scores in dice_per_expert.items():
            if isinstance(expert_scores, list):
                for expert_idx, score in enumerate(expert_scores):
                    self.writer.add_scalar(f'Personalization/dice_per_expert_{class_name}_expert_{expert_idx}', score, epoch)
                # è®°å½•å¹³å‡å€¼
                avg_score = sum(expert_scores) / len(expert_scores) if expert_scores else 0.0
                self.writer.add_scalar(f'Personalization/dice_per_expert_{class_name}_avg', avg_score, epoch)
        
        # è®°å½•å…¶ä»–ä¸ªæ€§åŒ–æŒ‡æ ‡
        dice_max = metrics.get('dice_max', {})
        dice_match = metrics.get('dice_match', {})
        for class_name in ['disc', 'cup', 'overall']:
            if class_name in dice_max:
                self.writer.add_scalar(f'Personalization/dice_max_{class_name}', dice_max[class_name], epoch)
            if class_name in dice_match:
                self.writer.add_scalar(f'Personalization/dice_match_{class_name}', dice_match[class_name], epoch)
        
        # è®°å½•GEDæŒ‡æ ‡
        if 'ged' in metrics:
            self.writer.add_scalar('Metrics/ged', metrics['ged'], epoch)
        
        # è®°å½•å­¦ä¹ ç‡
        self.writer.add_scalar('Learning_Rate', learning_rate, epoch)
    
    def close(self):
        self.writer.close()



# ==============================================================================
# æ—¥å¿—æ‰“å°è¾…åŠ©å‡½æ•°
# ==============================================================================

def log_epoch_summary(epoch, total_epochs, train_losses, val_metrics):
    """åœ¨æ§åˆ¶å°æ‰“å°æ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æ‘˜è¦ã€‚"""
    log_info(f"--- Epoch {epoch + 1}/{total_epochs} Summary ---", print_message=True)
    
    # è®°å½•è®­ç»ƒæŸå¤±ï¼ˆæ‰€æœ‰åˆ†é‡ï¼‰
    log_info(f"  - Avg Train Loss (Total): {train_losses.get('total_loss', 0):.6f}", print_message=True)
    for loss_name, loss_value in train_losses.items():
        if loss_name != 'total_loss':
            log_info(f"    - {loss_name}: {loss_value:.6f}", print_message=True)
    
    # è®°å½•è½¯ Dice åˆ†æ•°ï¼ˆæŒ‰ç±»åˆ«ï¼‰
    soft_dice = val_metrics.get('soft_dice', {})
    mean_dice = soft_dice.get('mean', 0)
    disc_dice = soft_dice.get('disc', 0)
    cup_dice = soft_dice.get('cup', 0)
    
    log_info(f"  - Validation Soft Dice Mean: {mean_dice:.6f}", print_message=True)
    log_info(f"    - Disc: {disc_dice:.4f}, Cup: {cup_dice:.4f}", print_message=True)
    
    # è®°å½•ä¸ªæ€§åŒ–æŒ‡æ ‡
    dice_per_expert = val_metrics.get('dice_per_expert', {})
    dice_max = val_metrics.get('dice_max', {})
    dice_match = val_metrics.get('dice_match', {})
    
    if dice_per_expert:
        log_info(f"  - Dice Per Expert:", print_message=True)
        for class_name, expert_scores in dice_per_expert.items():
            if isinstance(expert_scores, list) and expert_scores:
                avg_score = sum(expert_scores) / len(expert_scores)
                scores_str = ', '.join([f"{score:.4f}" for score in expert_scores])
                log_info(f"    - {class_name}: [{scores_str}] (avg: {avg_score:.4f})", print_message=True)
    
    if dice_max:
        log_info(f"  - Dice Max:", print_message=True)
        for class_name, score in dice_max.items():
            log_info(f"    - {class_name}: {score:.4f}", print_message=True)
    
    if dice_match:
        log_info(f"  - Dice Match:", print_message=True)
        for class_name, score in dice_match.items():
            log_info(f"    - {class_name}: {score:.4f}", print_message=True)
    
    # è®°å½•GEDæŒ‡æ ‡
    if 'ged' in val_metrics:
        log_info(f"  - GED: {val_metrics['ged']:.6f}", print_message=True)
    
    log_info("-" * 40, print_message=True)


def save_final_results(save_dir, best_metrics, last_metrics, config):
    """å°†æœ€ç»ˆçš„è®­ç»ƒç»“æœæ‘˜è¦ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶ã€‚"""
    path = os.path.join(save_dir, 'final_training_summary.txt')
    with open(path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("è®­ç»ƒå®Œæˆ - æœ€ç»ˆç»“æœæ‘˜è¦\n")
        f.write("="*80 + "\n\n")
        
        f.write(f"å®éªŒç›®å½•: {save_dir}\n")
        f.write(f"æ€»è®­ç»ƒè½®æ•°: {config['training']['epochs']}\n\n")

        f.write("--- æœ€ä½³EpochæŒ‡æ ‡ ---\n")
        f.write(f"æœ€ä½³è½®æ¬¡: {best_metrics.get('epoch', -1) + 1}\n")
        
        # Soft Dice æŒ‡æ ‡
        best_dice = best_metrics.get('metrics', {}).get('soft_dice', {})
        f.write(f"  - Soft Dice Mean: {best_dice.get('mean', 0):.6f}\n")
        f.write(f"  - Soft Dice Disc: {best_dice.get('disc', 0):.6f}\n")
        f.write(f"  - Soft Dice Cup: {best_dice.get('cup', 0):.6f}\n")
        
        # ä¸ªæ€§åŒ–æŒ‡æ ‡
        best_metrics_data = best_metrics.get('metrics', {})
        best_dice_max = best_metrics_data.get('dice_max', {})
        best_dice_match = best_metrics_data.get('dice_match', {})
        best_dice_per_expert = best_metrics_data.get('dice_per_expert', {})
        
        if best_dice_max:
            f.write(f"  - Dice Max Overall: {best_dice_max.get('overall', 0):.6f}\n")
            f.write(f"  - Dice Max Disc: {best_dice_max.get('disc', 0):.6f}\n")
            f.write(f"  - Dice Max Cup: {best_dice_max.get('cup', 0):.6f}\n")
        
        if best_dice_match:
            f.write(f"  - Dice Match Overall: {best_dice_match.get('overall', 0):.6f}\n")
            f.write(f"  - Dice Match Disc: {best_dice_match.get('disc', 0):.6f}\n")
            f.write(f"  - Dice Match Cup: {best_dice_match.get('cup', 0):.6f}\n")
        
        if best_dice_per_expert:
            f.write(f"  - Dice Per Expert:\n")
            for class_name, expert_scores in best_dice_per_expert.items():
                if isinstance(expert_scores, list) and expert_scores:
                    avg_score = sum(expert_scores) / len(expert_scores)
                    scores_str = ', '.join([f"{score:.4f}" for score in expert_scores])
                    f.write(f"    - {class_name}: [{scores_str}] (avg: {avg_score:.4f})\n")
        
        if 'ged' in best_metrics_data:
            f.write(f"  - GED: {best_metrics_data['ged']:.6f}\n")
        f.write("\n")
        
        f.write("--- æœ€åEpochæŒ‡æ ‡ ---\n")
        f.write(f"æœ€åè½®æ¬¡: {last_metrics.get('epoch', -1) + 1}\n")
        
        # Soft Dice æŒ‡æ ‡
        last_dice = last_metrics.get('metrics', {}).get('soft_dice', {})
        f.write(f"  - Soft Dice Mean: {last_dice.get('mean', 0):.6f}\n")
        f.write(f"  - Soft Dice Disc: {last_dice.get('disc', 0):.6f}\n")
        f.write(f"  - Soft Dice Cup: {last_dice.get('cup', 0):.6f}\n")
        
        # ä¸ªæ€§åŒ–æŒ‡æ ‡
        last_metrics_data = last_metrics.get('metrics', {})
        last_dice_max = last_metrics_data.get('dice_max', {})
        last_dice_match = last_metrics_data.get('dice_match', {})
        last_dice_per_expert = last_metrics_data.get('dice_per_expert', {})
        
        if last_dice_max:
            f.write(f"  - Dice Max Overall: {last_dice_max.get('overall', 0):.6f}\n")
            f.write(f"  - Dice Max Disc: {last_dice_max.get('disc', 0):.6f}\n")
            f.write(f"  - Dice Max Cup: {last_dice_max.get('cup', 0):.6f}\n")
        
        if last_dice_match:
            f.write(f"  - Dice Match Overall: {last_dice_match.get('overall', 0):.6f}\n")
            f.write(f"  - Dice Match Disc: {last_dice_match.get('disc', 0):.6f}\n")
            f.write(f"  - Dice Match Cup: {last_dice_match.get('cup', 0):.6f}\n")
        
        if last_dice_per_expert:
            f.write(f"  - Dice Per Expert:\n")
            for class_name, expert_scores in last_dice_per_expert.items():
                if isinstance(expert_scores, list) and expert_scores:
                    avg_score = sum(expert_scores) / len(expert_scores)
                    scores_str = ', '.join([f"{score:.4f}" for score in expert_scores])
                    f.write(f"    - {class_name}: [{scores_str}] (avg: {avg_score:.4f})\n")
        
        if 'ged' in last_metrics_data:
            f.write(f"  - GED: {last_metrics_data['ged']:.6f}\n")
        f.write("\n")
    log_info(f"æœ€ç»ˆç»“æœæ‘˜è¦å·²ä¿å­˜è‡³: {path}", print_message=True)